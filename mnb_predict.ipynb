{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Prediction\n",
    "This notebook uses a pre-trained model for predicting 8 subreddit class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Functions\n",
    "- Import libraries\n",
    "- Import stopwords\n",
    "- Set pandas options and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English and Tagalog stopwords...\n"
     ]
    }
   ],
   "source": [
    "# * Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "cleaning_regex = re.compile(r\"[^\\w\\s]\")\n",
    "current_path = str(pathlib.Path().resolve())\n",
    "\n",
    "model_dir = f'{current_path}\\\\models\\\\scratch\\\\'\n",
    "model_sklearn_dir = f'{current_path}\\\\models\\\\sklearn\\\\model\\\\'\n",
    "tfidf_dir = f'{current_path}\\\\models\\\\sklearn\\\\tfidf\\\\'\n",
    "\n",
    "# * Import stopwords\n",
    "print(f'Loading English and Tagalog stopwords...')\n",
    "eng_file_path = f'{current_path}\\\\stopwords\\\\english_stopwords.txt'\n",
    "tag_file_path = f'{current_path}\\\\stopwords\\\\tagalog_stopwords.txt'\n",
    "eng_stop = set()\n",
    "tag_stop = set()\n",
    "\n",
    "with open(eng_file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        eng_stop.add(line.replace('\\n', ''))\n",
    "\n",
    "with open(tag_file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tag_stop.add(line.replace('\\n', ''))\n",
    "\n",
    "# * Function implementation for predicting the class given the text\n",
    "def predict(token: str, classes: list[str], likelihoods_df: DataFrame) -> str:\n",
    "    \"\"\" Returns the predicted class \"\"\"\n",
    "    max_likelihood = {}\n",
    "\n",
    "    # Maximize the likelihood for each class\n",
    "    for c in classes:\n",
    "        max_likelihood[c] = 1\n",
    "\n",
    "        # Calculate the maximum likelihood estimate for a class\n",
    "        for i in token.split(' '):\n",
    "            if i in likelihoods_df.index:\n",
    "                # Sum the logarithm of probabilities (log-likelihood)\n",
    "                max_likelihood[c] += likelihoods_df.loc[i][c]\n",
    "    \n",
    "    # Yield the class that has the highest log-probability score\n",
    "    prediction = max(max_likelihood, key=max_likelihood.get)\n",
    "    return prediction\n",
    "\n",
    "# Prediction function using the loaded model and vectorizer\n",
    "def predict_sklearn(token: str, tfidf: TfidfVectorizer, model: MultinomialNB):\n",
    "    # Transform the input text using the loaded TF-IDF vectorizer\n",
    "    text_transformed = tfidf.transform([token])\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(text_transformed)\n",
    "    # Return the predicted subreddit\n",
    "    return prediction[0]\n",
    "\n",
    "classes = [\n",
    "    'DeepThoughts', \n",
    "    'CryptoCurrencies', \n",
    "    'askphilosophy', \n",
    "    'computerscience',\n",
    "    'LawSchool', \n",
    "    'Wallstreetbetsnew', \n",
    "    'PoliticalDiscussion', \n",
    "    'geopolitics'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Model from Scratch\n",
    "Multinomial Naive Bayes model trained on a corpus from 8 subreddit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing model...')\n",
    "model = pd.read_csv(f'{model_dir}mnb_model_scratch2.csv', index_col='term')\n",
    "model.sort_values(by=['computerscience'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LawSchool'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paste the body of text and/or title from a submission\n",
    "text = \"\"\"\n",
    "I always bomb cold calls in contracts, I’m coming to class prepared. But if the question isn’t something directly related to the brief or questions she gives us to do for each case, I usually can’t come up with an adequate answer in time. And so far majority of the time when I get cold called, it usually is a question about something else I don’t have “prepared”\n",
    "\n",
    "Has anyone had any experience with professors doing this? In the syllabus, it says they reserve the right to increase or lower a students grade based on “class preparation”\n",
    "\"\"\"\n",
    "\n",
    "# Clean the text\n",
    "tokenize = lambda x: ' '.join([token for token in word_tokenize(x) if len(token) > 2 and token not in eng_stop and token not in tag_stop])\n",
    "text = tokenize(re.sub(cleaning_regex, ' ', text.lower()))\n",
    "\n",
    "predict(text, classes, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Model using SKLearn\n",
    "Multinomial Naive Bayes model trained on a corpus from 8 subreddit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing model and TF-IDF...\n"
     ]
    }
   ],
   "source": [
    "print('Importing model and TF-IDF...')\n",
    "# Load the saved model\n",
    "model = joblib.load(f'{model_sklearn_dir}mnb_model_sklearn2.pkl')\n",
    "\n",
    "# Load the saved TF-IDF vectorizer\n",
    "tfidf = joblib.load(f'{tfidf_dir}mnb_tfidf_sklearn2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('computerscience')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paste the body of text and/or title from a submission\n",
    "text = \"\"\"\n",
    "Hi everyone i just got accepted into computer science and probably not changing it i do live in a third world country so there isnt that much interest in it so i think i have a good chance of becoming something so i have 3 questions what should i try to achieve in my 4 years of computer science to be at least somewhat above average and does computer science have physics or math?(My fav subjects) And is computer science generally hard?\n",
    "\"\"\"\n",
    "\n",
    "# Clean the text\n",
    "tokenize = lambda x: ' '.join([token for token in word_tokenize(x) if len(token) > 2 and token not in eng_stop and token not in tag_stop])\n",
    "tokens = tokenize(re.sub(cleaning_regex, ' ', text.lower()))\n",
    "\n",
    "predict_sklearn(tokens, tfidf, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RedditTextVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
